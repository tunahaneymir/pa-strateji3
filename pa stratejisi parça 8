# Ana Strateji - 8. Parca
# Learning & Complete System Integration

continuous_learning_loop:
  architecture:
    flow: "Real Market → PA Analysis → Setup Detection → RL Decision → Execute → Monitor → Exit → Outcome → RL Update → Repeat"
    description: "Continuous feedback loop with data-driven self-optimization"
  experience_buffer:
    capacity: 100000
    structure:
      market_data: [coin, timeframe]
      state_features: 40
      action: ["ENTER_FULL", "ENTER_REDUCED", "SKIP", "WAIT"]
      trade_info: [entry, stop, tp1, tp2, position_size, risk_percent]
      outcome: [pnl_percent, pnl_usd, duration, exit_reason]
      reward: outcome_score
      meta_flags: [setup_complete, fomo_detected, revenge_detected, emotional_state]
  learning_process:
    phase_1_data_collection:
      frequency: "Every trade"
      storage: "Experience buffer (100K)"
      collects: ["setup characteristics", "market context", "decision", "outcome", "behavioral flags"]
    phase_2_model_training:
      frequency: "Every 4 hours"
      batch_size: 128
      epochs: 5
      sample_size: 10000
      method: "Train PPO model using recent data"
    phase_3_performance_analysis:
      frequency: "Daily"
      metrics: ["win_rate", "profit_factor", "drawdown", "best_patterns", "parameter efficiency"]
      actions: ["adjust thresholds", "update ranges", "blacklist failing setups"]
    phase_4_parameter_optimization:
      frequency: "Weekly"
      method: "Grid search + RL feedback"
      targets: ["ZigZag depth", "Zone threshold", "ChoCH strength", "Risk multipliers", "Cooldowns"]

performance_tracking:
  metrics:
    overall: ["win_rate", "avg_win", "avg_loss", "profit_factor", "drawdown"]
    per_coin: ["BTCUSDT", "ETHUSDT", "SOLUSDT", "AVAXUSDT"]
    per_timeframe: {"4H": 0.68, "1H": 0.64, "15M": 0.61}
    behavioral: ["fomo_blocks", "revenge_blocks", "overtrading_blocks"]
    market_regime: {"trending": 0.71, "ranging": 0.52, "breakout": 0.65}
  example_dashboard:
    total_trades: 320
    win_rate: "65%"
    profit_factor: 2.15
    total_pnl_percent: "+47.5%"
    sharpe_ratio: 1.68
    max_drawdown: "8.5%"
    top_coin: "BTCUSDT 70% WR"
    best_tf: "4H 72% WR"
    confidence: 0.78
    stress: 0.18
    state: "Optimal trading condition"

adaptive_threshold_learning:
  initial:
    zone_quality: 4
    choch_strength: 0.4
    setup_score: 40
  learning_phase:
    analysis: "Higher quality zones → higher win rate"
    decision: "Raise thresholds to zone_quality 6, choch 0.5, setup_score 50"
  optimized:
    global: {zone_quality: 6, choch_strength: 0.5, setup_score: 50}
    coin_specific:
      BTCUSDT: {zone_quality: 5, choch_strength: 0.5, setup_score: 50}
      ETHUSDT: {zone_quality: 6, choch_strength: 0.5, setup_score: 55}
      SOLUSDT: {zone_quality: 7, choch_strength: 0.6, setup_score: 60}
  dynamic_adjustment_logic:
    if_winrate_gt_75: {action: "LOWER_THRESHOLDS", reason: "Too selective"}
    if_winrate_lt_55: {action: "RAISE_THRESHOLDS", reason: "Too liberal"}
    else: {action: "MAINTAIN", reason: "Optimal performance range"}

full_system_architecture:
  agents:
    - name: "Agent 1: Coin Selection"
      functions: ["Volume", "Liquidity", "Volatility", "Top 5-10 coins"]
    - name: "Agent 2: RL Trading Bot"
      modules:
        - "PA Detection (EMA, Zone, ChoCH, Fibonacci)"
        - "Setup Scoring (0-100)"
        - "4-Gate Validation"
        - "Behavioral Protection"
        - "RL Decision Engine (PPO)"
    - name: "Agent 3: Risk Management"
      modules:
        - "Position sizing"
        - "Adaptive multipliers"
        - "Stop loss / TP levels"
        - "Correlation check"
        - "Execution"
  trade_management:
    includes: ["TP tracking", "Trailing stop", "Re-entry after stop"]
  post_trade_learning:
    includes: ["Outcome scoring", "Experience update", "RL training", "Zone memory update", "Cooldown & performance tracking"]
  loop: "Continuous feedback to Agent 1"

learning_phases:
  phase_1:
    name: "Exploration (0-3 months)"
    thresholds: {zone_quality: 4, choch: 0.4, score: 40}
    risk: 2
    goal: "Collect 5000+ trades"
    expected_wr: "55-60%"
  phase_2:
    name: "Optimization (3-6 months)"
    thresholds: {zone_quality: 6, choch: 0.5, score: 50}
    risk: "1-2%"
    expected_wr: "62-67%"
  phase_3:
    name: "Mastery (6-12 months)"
    thresholds: {adaptive: true, coin_specific: true}
    expected_wr: "68-73%"
  phase_4:
    name: "Autonomous Excellence (12+ months)"
    behavior: "Self-optimizing, discovers new patterns"
    expected_wr: "70-75%"

performance_targets:
  phase_1: {win_rate: "55-60%", profit_factor: "1.5+", sharpe: "0.8+", max_dd: "<15%"}
  phase_2: {win_rate: "62-67%", profit_factor: "1.8+", sharpe: "1.2+", max_dd: "<12%"}
  phase_3: {win_rate: "68-73%", profit_factor: "2.2+", sharpe: "1.6+", max_dd: "<10%"}
  phase_4: {win_rate: "70-75%", profit_factor: "2.5+", sharpe: "2.0+", max_dd: "<8%"}

implementation_roadmap:
  total_duration: "12 weeks"
  phases:
    - week_1_2: "Core PA Module (Trend, Zone, ChoCH, Fib)"
    - week_3: "Setup Scoring & Gate Validation"
    - week_4: "Risk Management Integration"
    - week_5: "Behavioral Protection (FOMO, Revenge, Overtrading)"
    - week_6_7: "RL Agent Integration (PPO)"
    - week_8: "Learning System & Metrics"
    - week_9: "3-Agent Integration"
    - week_10_12: "Testing & Deployment"
  testing_stages:
    - backtesting: "2-3y historical data"
    - paper_trading: "1-2 months live data"
    - small_capital: "$1-2K real test"
    - full_deployment: "Scaled and monitored"

final_summary:
  system:
    architecture: "3-Agent Hybrid (PA + RL)"
    components: 8
    timeframe_structure: ["4H Trend", "1H Zone", "15M Entry"]
    gate_layers: 4
    behavioral_protection: ["FOMO", "Revenge", "Overtrading"]
    rl_core: "PPO algorithm, continuous training"
    learning_duration: "12+ months"
  expected_results:
    year_1_return: "+100–150%"
    win_rate_progression: "55% → 70–75%"
    sharpe_ratio: "2.0+"
    max_drawdown: "<10%"
  philosophy:
    - "Patience over speed"
    - "Discipline over emotion"
    - "Quality over quantity"
    - "Long-term consistency"
